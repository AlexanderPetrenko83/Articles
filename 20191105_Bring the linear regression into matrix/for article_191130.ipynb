{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Приводим уравнение линейной регрессии в матричный вид\n",
    "\n",
    "Цель статьи - оказание поддержки начинающим датасайнтистам. В $предыдущей$ $статье$ мы на пальцах разобрали три способа решения уравнения линейной регрессии: аналитическое решение, градиентный спуск, стохастический градиентный спуск. Тогда для аналитического решения мы применили формулу $X^T X \\vec{w} = X^T \\vec{y}$. В этой статье, как следует из заголовка, мы обоснуем применение данной формулы или другими словами, самостоятельно ее выведем. \n",
    "\n",
    "Почему имеет смысл уделить повышенное внимание к формуле $X^T X \\vec{w} = X^T \\vec{y}$ ? \n",
    "\n",
    "Именно с матричного уравнения в большинстве случаев начинается знакомство с линейной регрессией. При этом, подробные выкладки того, как формула была выведена, встречаются редко.\n",
    "\n",
    "Например, на курсах по машинному обучению от Яндекса, когда слушателей знакомят с регуляризацией, то предлагают воспользоваться функциями из библиотеки sklearn, при этом ни слова не упоминается о матричном представлении алгоритма. Именно в этот момент у некоторых слушателей может появится желание разобраться в этом вопросе подробнее - написать код без использования готовых функций. А для этого, надо сначала представить уравнение с регуляризатором в матричном виде. Данная статья, как раз, позволит желающим овладеить такими умениями. Приступим."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Исходные условия\n",
    "#### Целевые показатели\n",
    "У нас имеется ряд значений целевого показателя. Например, целевым показателем может быть цена на какой-либо актив: нефть, золото, пшеница, доллар и т.д. При этом, под рядом значений целевого показателя мы понимаем количество наблюдений. Такими наблюдениями могут быть, например, ежемесячные цены на нефть за год, то есть у нас будет 12 значений целевого показателя. Начнем вводить обозначения. Обозначим каждое значение целевого показателя как $y_i$. Всего мы имеем $n$ наблюдений, а значит можно представить наши наблюдения как $y_1$, $y_2$, $y_3$ ... $y_n$.\n",
    "\n",
    "#### Регрессоры\n",
    "Будем считать, что существуют факторы, которые в определенной степени объясняют значения целевого показателя. Например, на курс пары доллар/рубль сильное влияние оказывает цена на нефть, ставка ФРС и др. Такие факторы называются регрессорами. При этом, каждому значению целевого показателя должно соответствовать значение регрессора, то есть, если у нас имеется 12 целевых показателей за каждый месяц в 2018 году, то и значений регрессоров у нас тоже должно быть 12 за тот же период. Обозначим значения каждого регрессора через $x_i$: $x_1$, $x_2$, $x_3$ ... $x_n$. Пусть в нашем случае имеется $k$ регрессоров (т.е. $k$ факторов, которые оказывают влияние на значения целевого показателя). Значит наши регрессоры можно представить следующим образом: для 1-го регрессора (например, цена на нефть): $x_{11}$, $x_{12}$, $x_{13}$ ... $x_{1n}$, для 2-го регрессора (например, ставка ФРС): $x_{21}$, $x_{22}$, $x_{23}$ ... $x_{2n}$, для \"$k$-го\" регрессора: $x_{k1}$, $x_{k2}$, $x_{k3}$ ... $x_{kn}$\n",
    "\n",
    "#### Зависимость целевых показателей от регрессоров\n",
    "Предположим, что зависимость целевого показателя $y_i$ от регрессоров \"$i$-го\" наблюдения может быть выражена через уравнение линейной регрессии вида:\n",
    "$$ f(w,x_i) = w_0 + w_1 x_{1i} + ... + w_k x_{ki} $$\n",
    "\n",
    ", где $x_i$ - \"$i$-ое\" значение регрессора от 1 до $n$,\n",
    "\n",
    "$k$ - количество регрессоров от 1 до $k$\n",
    "\n",
    "$w$ - угловые коэффициенты, которые представляют величину, на которую изменится расчетный целевой показатель в среднем при изменении регрессора. \n",
    "\n",
    "Другими словами, мы для каждого (за исключением $w_0$) регрессора определяем \"свой\" коэффициент $w$, затем перемножаем коэффициенты на значения регрессоров \"$i$-го\" наблюдения, в результате получаем некое приближение \"$i$-го\" целевого показателя.\n",
    "\n",
    "Следовательно, нам нужно подобрать такие коэффициенты $w$, при которых значения нашей апроксимирующей функции $f(w,x_i)$ будут расположены максимально близко к значениям целевых показателей.\n",
    "\n",
    "#### Оценка качества апроксиммирующей функции\n",
    "Будем определять оценку качества апроксимирующей функции методом наименьших квадратов. Функция оценки качества в таком случае примет следующий вид:\n",
    "$$Err= \\sum\\limits_{i=1}^n(y_i-f(x_i))^2 \\rightarrow min$$\n",
    "\n",
    "Нам требуется подобрать такие значения коэффициентов $w$, при которых значение $Err$ будет наименьшим."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Переводим уравнение в матричный вид\n",
    "#### Векторное представление\n",
    "Для начала, чтобы облегчить себе жизнь, следует обратить внимание на уравнение линейной регрессии и заметить, что первый коэффициент $w_0$ не умножается ни на один регрессор. При этом, когда мы переведем данные в матричный вид, вышеобозначенное обстоятельство будет серьезно осложнять расчеты. В этой связи предлагается ввести еще один регрессор для первого коэффициента $w_0$ и приравнять его единице. Вернее, каждое \"$i$-ое\" значение этого регрессора приравнять единице - ведь при умножении на единицу у нас с точки зрения результата вычислений ничего не изменится, а с точки зрения правил произведения матриц, существенно сократятся наши мучения.  \n",
    "\n",
    "Теперь, на некоторое время, с целью упрощения материала, предположим, что у нас только одно \"$i$-ое\" наблюдение. Тогда, представим значения регрессоров \"$i$-ого\" наблюдения в качестве вектора $ \\vec{x_i}$. Вектор $ \\vec{x_i}$ имеет размерность $(k \\times 1)$, то есть $k$ строк и 1 столбец:\n",
    "$$\\vec{x_i} = \\begin{pmatrix} x_{0i} \\\\ x_{1i} \\\\ ... \\\\ x_{ki} \\end{pmatrix} \\qquad$$\n",
    "\n",
    "Искомые коэффициенты представим в виде вектора $ \\vec{w}$, имеющего размерность $(k \\times 1)$:\n",
    "$$\\vec{w}=\\begin{pmatrix} w_0 \\\\ w_1 \\\\ ... \\\\ w_k \\end{pmatrix} \\qquad$$\n",
    "\n",
    "Уравнение линейной регрессии для \"$i$-го\" наблюдения примет вид: \n",
    "$$ f(w,x_i) = \\vec{x_i}^T \\vec{w}$$\n",
    "\n",
    "Функция оценки качества линейной модели примет вид:\n",
    "$$Err= \\sum\\limits_{i=1}^n(y_i-\\vec{x_i}^T \\vec{w})^2 \\rightarrow min$$\n",
    "\n",
    "Обратим внимание, что в соответствии с правилами умножения матриц, нам потребовалось транспонировать вектор $ \\vec{x_i}$.\n",
    "\n",
    "#### Матричное представление\n",
    "В результате умножения векторов, мы получим число: $(1 \\times k) \\centerdot (k \\times 1) = 1 \\times 1$, что и следовало ожидать. Это число и есть приближение \"$i$-го\" целевого показателя. Но нам-то нужно приближение не одного значения целевого показателя, а всех. Для этого запишем все \"$i$-ые\" регрессоры в формате матрицы $X$. Полученная матрица имеет размерность $(n \\times k)$:\n",
    "$$X=\\begin{pmatrix} x_{00} & x_{01} & ... & x_{0k} \\\\ x_{10} & x_{11} & ... & x_{1k} \\\\ ... & ... & ... & ... \\\\ x_{n0} & x_{n1} & ... & x_{nk} \\end{pmatrix} \\qquad$$\n",
    "\n",
    "Теперь уравнение линейной регрессии примет вид:\n",
    "$$ f(w,X) = X \\vec{w}$$\n",
    "\n",
    "Обозначим значения целевых показателей (все $y_i$) за вектор $\\vec{y}$ размерностью $(n \\times 1)$:\n",
    "$$\\vec{y} = \\begin{pmatrix} y_{0} \\\\ y_{1} \\\\ ... \\\\ y_{n} \\end{pmatrix} \\qquad$$\n",
    "\n",
    "Теперь мы можем записать в матричном формате уравнение оценки качества линейной модели:\n",
    "$$ Err = (X \\vec{w} - \\vec{y})^2\\rightarrow min $$\n",
    "\n",
    "Собственно, из этой формулы далее получают известную нам формулу $X^T X w^T = X^T y$\n",
    "\n",
    "Как это делается? Раскрываются скобки, проводится дифференцирование, преобразуются полученные выражения и т.д., и именно этим мы сейчас и займемся."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Матричные преобразования\n",
    "#### Раскроем скобки\n",
    "$(X \\vec{w} - \\vec{y})^2 = (X \\vec{w} - \\vec{y})^T(X \\vec{w} - \\vec{y})$\n",
    "\n",
    "$=(X\\vec{w})^TX\\vec{w} - \\vec{y}^TX\\vec{w} - (X\\vec{w})^T\\vec{y} + \\vec{y}^T\\vec{y}$\n",
    "#### Подготовим уравнение для дифференцирования\n",
    "Для этого проведем некоторые преобразования. В последующих расчетах нам будет удобнее, если вектор $\\vec{w}^T$ будет представлен в начале каждого произведения в уравнении.\n",
    "##### Преобразование 1\n",
    "$\\vec{y}^TX\\vec{w} = (X\\vec{w})^T\\vec{y} = \\vec{w}^TX^T\\vec{y}$\n",
    "\n",
    "Как это получилось? Для ответа на этот вопрос достаточно посмотреть на размеры умножаемых матриц и увидеть, что на выходе мы получаем число или иначе $const$.\n",
    "\n",
    "Запишем размеры матричных выражений.\n",
    "\n",
    "$\\vec{y}^TX\\vec{w}$ : $(1 \\times n) \\centerdot (n \\times k) \\centerdot (k \\times 1) = (1 \\times 1) = const$\n",
    "\n",
    "$(X\\vec{w})^T\\vec{y}$ : $((n \\times k) \\centerdot (k \\times 1))^T \\centerdot (n \\times 1) = (1 \\times n) \\centerdot (n \\times 1) = (1 \\times 1) = const$\n",
    "\n",
    "$\\vec{w}^TX^T\\vec{y}$ : $(1 \\times k) \\centerdot (k \\times n) \\centerdot (n \\times 1) = (1 \\times 1) = const$\n",
    "##### Преобразование 2\n",
    "$(X\\vec{w})^TX\\vec{w} = \\vec{w}^TX^TX\\vec{w}$\n",
    "\n",
    "Распишем аналогичено преобразованию 1\n",
    "\n",
    "$(X\\vec{w})^TX\\vec{w}$ : $((n \\times k) \\centerdot (k \\times 1))^T \\centerdot (n \\times k) \\centerdot (k \\times 1) = (1 \\times 1) = const$\n",
    "\n",
    "$\\vec{w}^TX^TX\\vec{w}$ : $(1 \\times k) \\centerdot (k \\times n) \\centerdot (n \\times k) \\centerdot (k \\times 1) = (1 \\times 1) = const$\n",
    "\n",
    "На выходе получаем уравнение, которое нам предстоит продифференцировать:\n",
    "$$Err = \\vec{w}^TX^TX\\vec{w} - 2\\vec{w}^TX^T\\vec{y} + \\vec{y}^T\\vec{y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дифференцируем функцию оценки качества модели\n",
    "Продифференцируем по вектору $\\vec{w}$:\n",
    "\n",
    "$$\\frac{d(\\vec{w}^TX^TX\\vec{w} - 2\\vec{w}^TX^T\\vec{y} + \\vec{y}^T\\vec{y})}{d\\vec{w}}$$\n",
    "$(\\vec{w}^TX^TX\\vec{w})' - (2\\vec{w}^TX^T\\vec{y})' + (\\vec{y}^T\\vec{y})' = 0$\n",
    "\n",
    "$2X^TX\\vec{w} - 2X^T\\vec{y} + 0 = 0$\n",
    "\n",
    "$X^TX\\vec{w}=X^T\\vec{y}$\n",
    "\n",
    "Вопросов почему $(\\vec{y}^T\\vec{y})' = 0$ быть не должно, а вот операции по определению производных в двух других выражениях мы разберем подробнее.\n",
    "#### Дифференцирование 1\n",
    "Раскроем дифференцирование:\n",
    "$\\frac{d(\\vec{w}^TX^TX\\vec{w})}{d\\vec{w}} = 2X^TX\\vec{w}$\n",
    "\n",
    "Для того, чтобы определить производную от матрицы или вектора требуется посмотреть, что у них там внутри. Смотрим:\n",
    "\n",
    "$\\vec{w}^T=\\begin{pmatrix} w_0 & w_1 & ... & w_k \\end{pmatrix} \\qquad$\n",
    "\n",
    "$\\vec{w}=\\begin{pmatrix} w_0 \\\\ w_1 \\\\ ... \\\\ w_k \\end{pmatrix} \\qquad$\n",
    "\n",
    "$X^T = \\begin{pmatrix} x_{00} & x_{10} & ... & x_{n0} \\\\ x_{01} & x_{11} & ... & x_{n1} \\\\ ... & ... & ... & ... \\\\ x_{0k} & x_{1k} & ... & x_{nk} \\end{pmatrix} \\qquad$ \n",
    "$X = \\begin{pmatrix} x_{00} & x_{01} & ... & x_{0k} \\\\ x_{10} & x_{11} & ... & x_{1k} \\\\ ... & ... & ... & ... \\\\ x_{n0} & x_{n1} & ... & x_{nk} \\end{pmatrix} \\qquad$\n",
    "\n",
    "Обозначим произведение матриц $X^TX$ через матрицу $A$. Матрица $A$ квадратная и более того, она симметричная. Эти свойства нам пригодятся далее, запомним их. Матрица $A$ имеет размерность $(k \\times k)$:\n",
    "\n",
    "$A = \\begin{pmatrix} a_{00} & a_{01} & ... & a_{0k} \\\\ a_{10} & a_{11} & ... & a_{1k} \\\\ ... & ... & ... & ... \\\\ a_{k0} & a_{k1} & ... & a_{kk} \\end{pmatrix} \\qquad$\n",
    "\n",
    "Теперь наша задача правильно перемножить вектора на матрицу и не получить \"дважды два пять\", поэтому сосредоточимся и будем предельно внимательны.\n",
    "\n",
    "$\\vec{w}^TA\\vec{w}=\\begin{pmatrix} w_0 & w_1 & ... & w_k \\end{pmatrix} \\qquad \\times \\begin{pmatrix} a_{00} & a_{01} & ... & a_{0k} \\\\ a_{10} & a_{11} & ... & a_{1k} \\\\ ... & ... & ... & ... \\\\ a_{k0} & a_{k1} & ... & a_{kk} \\end{pmatrix} \\qquad \\times \\begin{pmatrix} w_0 \\\\ w_1 \\\\ ... \\\\ w_k \\end{pmatrix} \\qquad = $\n",
    "\n",
    "$= \\begin{pmatrix} w_0a_{00}+w_1a_{10}+...+w_ka_{k0} & ... & w_0a_{0k}+w_1a_{1k}+...+w_ka_{kk} \\end{pmatrix} \\times \\begin{pmatrix} w_0 \\\\ w_1 \\\\ ... \\\\ w_k \\end{pmatrix} \\qquad = $\n",
    "\n",
    "$= \\begin{pmatrix} (w_0a_{00}+w_1a_{10} +...+w_ka_{k0})w_0 \\mkern 10mu + \\mkern 10mu ... \\mkern 10mu + \\mkern 10mu (w_0a_{0k}+w_1a_{1k}+...+w_ka_{kk})w_k \\end{pmatrix} = $\n",
    "\n",
    "$= w_0^2a_{00}+w_1a_{10}w_0+w_ka_{k0}w_0 \\mkern 10mu + \\mkern 10mu ... \\mkern 10mu + \\mkern 10mu w_0a_{0k}w_k+w_1a_{1k}w_k+...+w_k^2a_{kk} $\n",
    "\n",
    "Однако, замысловатое выражение у нас получилось! На самом деле мы получили число - скаляр. И теперь, уже по-настоящему, переходим к дифференцированию. Необходимо найти производную полученного выражения по каждому коэффициенту $w_0$ $w_1$ $...$ $w_k$ и получить на выходе вектор размерности $(k \\times 1)$. На всякий случай распишу процедуры по действиям:\n",
    "* продифференцируем по $w_o$, получим: $2w_0a_{00}+w_1a_{10}+w_2a_{20} + ... + w_ka_{k0}+a_{01}w_1+a_{02}w_2+...+a_{0k}w_{k}$\n",
    "* продифференцируем по $w_1$, получим: $w_0a_{01}+2w_1a_{11}+w_2a_{21} + ... + w_ka_{k1}+a_{10}w_0+a_{12}w_2+...+a_{1k}w_{k}$\n",
    "* продифференцируем по $w_k$, получим: $w_0a_{0k}+w_1a_{1k}+w_2a_{2k} + ... + w_{(k-1)}a_{(k-1)k}+a_{k0}w_0+a_{k1}w_1+a_{k2}w_2+...+2w_ka_{kk}$\n",
    "\n",
    "На выходе, обещанный вектор размером $(k \\times 1)$:\n",
    "\n",
    "$$\\begin{pmatrix} 2w_0a_{00}+w_1a_{10}+w_2a_{20} + ... + w_ka_{k0}+a_{01}w_1+a_{02}w_2+...+a_{0k}w_{k} \\\\ w_0a_{01}+2w_1a_{11}+w_2a_{21} + ... + w_ka_{k1}+a_{10}w_0+a_{12}w_2+...+a_{1k}w_{k} \\\\ ... \\\\ ... \\\\ ... \\\\ w_0a_{0k}+w_1a_{1k}+w_2a_{2k} + ... + w_{(k-1)}a_{(k-1)k}+a_{k0}w_0+a_{k1}w_1+a_{k2}w_2+...+2w_ka_{kk} \\end{pmatrix}$$\n",
    "\n",
    "Если присмотреться к вектору повнимательнее, то можно заметить, что левые и соответствующие правые элементы вектора можно сгруппировать таким образом, что в итоге из представленного вектора можно выделить вектор $\\vec{w}$ размера $(k \\times 1)$. Например, $w_1a_{10}$ (левый элемент верхней строчки вектора) $+$ $a_{01}w_1$ (правый элемент верхней строчки вектора) можно представить как $w_1(a_{10}+a_{01})$, а $w_2a_{20}+a_{02}w_2$ - как $w_2(a_{20}+a_{02})$ и т.д. по каждой строчке. Сгруппируем:\n",
    "\n",
    "$$\\begin{pmatrix} 2w_0a_{00}+w_1(a_{10}+a_{01})+w_2(a_{20}+a_{02})+...+w_k(a_{k0}+a_{0k}) \\\\ w_0(a_{01}+a_{10})+2w_1a_{11}+w_2(a_{21}+a_{12})+...+w_k(a_{k1}+a_{1k}) \\\\ ... \\\\ ... \\\\ ... \\\\ w_0(a_{0k}+a_{k0})+w_1(a_{1k}+a_{k1})+w_2(a_{2k}+a_{k2})+...+2w_ka_{kk} \\end{pmatrix}$$\n",
    "\n",
    "Вынесем вектор $\\vec{w}$ и на выходе получим:\n",
    "\n",
    "$$\\begin{pmatrix} 2a_{00} & a_{10}+a_{01} & a_{20}+a_{02} &...& a_{k0}+a_{0k} \\\\ \n",
    "a_{01}+a_{10} & 2a_{11} & a_{21}+a_{12} &...& a_{k1}+a_{1k} \\\\\n",
    "...&...&...&...&...  \\\\ ...&...&...&...&... \\\\ ...&...&...&...&... \\\\ \n",
    "a_{0k}+a_{k0} & a_{1k}+a_{k1} & a_{2k}+a_{k2} &...& 2a_{kk} \\end{pmatrix} \n",
    "\\times \n",
    "\\begin{pmatrix} w_0 \\\\ w_1 \\\\ ... \\\\ ... \\\\ ... \\\\ w_k \\end{pmatrix} \\qquad$$\n",
    "\n",
    "Теперь, присмотримся к получившейся матрице. Матрица представляет собой сумму двух матриц: $A+A^T$:\n",
    "\n",
    "$$\\begin{pmatrix} a_{00} & a_{01} & a_{02} &...& a_{0k} \\\\ \n",
    "a_{10} & a_{11} & a_{12} &...& a_{1k} \\\\\n",
    "...&...&...&...&...  \\\\ \n",
    "a_{k0} & a_{k1} & a_{k2} &...& a_{kk} \\end{pmatrix}\n",
    "+\n",
    "\\begin{pmatrix}\n",
    "a_{00} & a_{10} & a_{20} &...& a_{k0} \\\\ \n",
    "a_{01} & a_{11} & a_{21} &...& a_{k1} \\\\\n",
    "...&...&...&...&...  \\\\ \n",
    "a_{0k} & a_{1k} & a_{2k} &...& a_{kk} \\end{pmatrix} \\qquad$$\n",
    "\n",
    "Вспомним, что несколько ранее, мы отметили одно важное свойство матрицы $A$ - она симметричная. Исходя из этого свойства, мы можем с уверенностью заявить, что выражение $A + A^T$ равняется $2A$. Это легко проверить, раскрыв поэлементно произведение матриц $X^TX$. Мы не будем делать этого здесь, желающие могут провести проверку самостоятельно. \n",
    "\n",
    "Вернемся к нашему выражению. После наших преобразований оно получилось таким, каким мы и хотели его увидеть:\n",
    "\n",
    "$$(A+A^T) \\times \\begin{pmatrix} w_0 \\\\ w_1 \\\\ ... \\\\ w_k \\end{pmatrix} \\qquad = 2A \\vec{w} = 2X^TX\\vec{w}$$\n",
    "\n",
    "Итак, с первым дифференцированием мы справились. Переходим ко второму выражению.\n",
    "\n",
    "#### Дифференцирование 2\n",
    "$\\frac{d(2\\vec{w}^TX^T\\vec{y})}{d\\vec{w}} = 2X^T\\vec{y}$\n",
    "\n",
    "Пойдем по протоптанной дорожке. Она будет намного короче предыдущей, так что не уходите далеко от экрана.\n",
    "\n",
    "Раскроем поэлементно вектора и матрицу:\n",
    "\n",
    "$\\vec{w}^T=\\begin{pmatrix} w_0 & w_1 & ... & w_k \\end{pmatrix} \\qquad$\n",
    "\n",
    "$X^T = \\begin{pmatrix} x_{00} & x_{10} & ... & x_{n0} \\\\ x_{01} & x_{11} & ... & x_{n1} \\\\ ... & ... & ... & ... \\\\ x_{0k} & x_{1k} & ... & x_{nk} \\end{pmatrix} \\qquad$ \n",
    "\n",
    "$\\vec{y}=\\begin{pmatrix} y_0 \\\\ y_1 \\\\ ... \\\\ y_n \\end{pmatrix} \\qquad$\n",
    "\n",
    "На время уберем из расчетов двойку - она большой роли не играет, потом вернем ее на место. Перемножим вектора на матрицу. В первую очередь умножим матрицу $X^T$ на вектор $\\vec{y}$, здесь у нас нет никаких ограничений. Получим вектор размера $(k \\times 1)$:\n",
    "$$\\begin{pmatrix} \n",
    "x_{00}y_0+x_{10}y_1+...+x_{n0}y_n \\\\\n",
    "x_{01}y_0+x_{11}y_1+...+x_{n1}y_n \\\\\n",
    "... \\\\\n",
    "x_{0k}y_0+x_{1k}y_1+...+x_{nk}y_n\n",
    "\\end{pmatrix} \\qquad$$\n",
    "\n",
    "Выполним следующее действие - умножим вектор $\\vec{w}$ на полученный вектор. На выходе нас будет ждать число:\n",
    "\n",
    "$$\\begin{pmatrix} w_0(x_{00}y_0+x_{10}y_1+...+x_{n0}y_n)+\n",
    "w_1(x_{01}y_0+x_{11}y_1+...+x_{n1}y_n) \\mkern 10mu +\n",
    "\\mkern 10mu ... \\mkern 10mu + \\mkern 10mu\n",
    "w_k(x_{0k}y_0+x_{1k}y_1+...+x_{nk}y_n)\n",
    "\\end{pmatrix} \\qquad$$\n",
    "\n",
    "Его то мы и продифференцируем. На выходе получим вектор размерности $(k \\times 1)$:\n",
    "\n",
    "$$\\begin{pmatrix} \n",
    "x_{00}y_0+x_{10}y_1+...+x_{n0}y_n \\\\\n",
    "x_{01}y_0+x_{11}y_1+...+x_{n1}y_n \\\\\n",
    "... \\\\\n",
    "x_{0k}y_0+x_{1k}y_1+...+x_{nk}y_n\n",
    "\\end{pmatrix} \\qquad$$\n",
    "\n",
    "Что-то напоминает? Все верно! Это произведение матрицы $X^T$ на вектор $\\vec{y}$. \n",
    "\n",
    "Таким образом, второе дифференцирование успешно завершено."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вместо заключения\n",
    "Теперь мы знаем, как получилось равенство $X^T X \\vec{w} = X^T \\vec{y}$.\n",
    "\n",
    "Напоследок опишем быстрый путь преобразований основных формул:\n",
    "\n",
    "##### Оценим качество модели в соответствии с методом наименьших квадратов:\n",
    "\n",
    "$\\sum\\limits_{i=1}^n(y_i-f(x_i))^2 \\mkern 20mu = \\mkern 20mu \\sum\\limits_{i=1}^n(y_i-\\vec{x_i}^T \\vec{w})^2=$\n",
    "\n",
    "$= (X \\vec{w} - \\vec{y})^2 \\mkern 20mu = \\mkern 20mu (X \\vec{w} - \\vec{y})^T(X \\vec{w} - \\vec{y}) \n",
    "\\mkern 20mu = \\mkern 20mu \\vec{w}^TX^TX\\vec{w} - 2\\vec{w}^TX^T\\vec{y} + \\vec{y}^T\\vec{y}$\n",
    "\n",
    "##### Дифференцируем полученное выражение:\n",
    "\n",
    "$\\frac{d(\\vec{w}^TX^TX\\vec{w} - 2\\vec{w}^TX^T\\vec{y} + \\vec{y}^T\\vec{y})}{d\\vec{w}}=$\n",
    "\n",
    "$=2X^TX\\vec{w} - 2X^T\\vec{y} = 0$\n",
    "\n",
    "$X^TX\\vec{w}=X^T\\vec{y}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
